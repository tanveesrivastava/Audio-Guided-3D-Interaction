# ğŸ—£ï¸ Offline 3D-Avatar-Based Conversational Agent Powered by On-Device LLM

> Real-time, private, and cloud-free voice assistant with speech recognition, language understanding, TTS, and animated 3D avatar lip syncâ€”fully operational on local hardware.

---

https://github.com/user-attachments/assets/33fbe515-e368-4937-9358-eb74b72ef213

## ğŸš€ Overview

This project presents an end-to-end pipeline for building a **real-time, LLM-powered conversational agent** that runs entirely **offline** on consumer-grade hardware. It integrates:

- On-device **speech-to-text (STT)**
- A **quantized large language model (LLM)** for natural language understanding and generation
- **Text-to-speech (TTS)** synthesis
- A **3D avatar** with synchronized **lip movements**

By eliminating reliance on cloud-based infrastructure, this system enables low-latency, privacy-preserving user interaction, particularly useful in remote, embedded, and bandwidth-constrained environments.

---

## ğŸ¯ Objectives

- ğŸ’¬ Enable real-time spoken conversations using an LLM entirely offline.
- ğŸ›¡ï¸ Ensure user **privacy** and **data sovereignty** by avoiding cloud dependencies.
- ğŸ­ Deliver rich **HCI experiences** using a 3D animated avatar.
- ğŸ“¦ Showcase the feasibility of full AI conversational pipelines on **consumer-grade hardware**.

---

## âœ¨ Features

- ğŸ”Š Real-time speech input (STT)
- ğŸ§  Natural language understanding via quantized LLM
- ğŸ—£ï¸ Text-to-speech voice generation
- ğŸ§ 3D avatar with dynamic facial animation and lip sync
- âš™ï¸ Fully offline operation â€” no internet or cloud required

---

## ğŸ§© Architecture

```
[Microphone] â†’ [STT Engine] â†’ [Quantized LLM] â†’ [TTS Engine] â†’ [3D Avatar + Lip Sync]
```

---

## ğŸ› ï¸ Tech Stack

- ğŸ§  **LLM**: [GGUF-based quantized models](https://github.com/ggerganov/llama.cpp) (e.g., LLaMA, Mistral, Gemma)
- ğŸ—£ï¸ **STT**: Whisper.cpp, Vosk, or Mozilla DeepSpeech (local)
- ğŸ”Š **TTS**: eSpeak NG, Coqui TTS, or Bark (offline)
- ğŸ‘¤ **3D Avatar**: Unity/Unreal/Blender-based rendering with lip sync
- âš™ï¸ **Programming Languages**: Python, C++, Unity C#

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/offline-3d-llm-agent.git
cd offline-3d-llm-agent
# Follow installation steps for each component (STT, LLM, TTS, Avatar)
```

> ğŸ“Œ Detailed installation instructions for each module are available in the respective subfolders.

---

## ğŸ§ª Demo

Coming soon! ğŸ‘€  
Check the [`demo/`](./demo) folder for sample recordings, avatar videos, and performance stats.

---

## ğŸ“š Research Motivation

This project addresses a critical gap in the deployment of privacy-first conversational AI systems. While cloud-based systems dominate todayâ€™s assistants (Siri, Alexa, ChatGPT), they introduce:

- ğŸš« Data privacy concerns  
- ğŸŒ Network latency  
- ğŸ“¡ Inaccessibility in offline/remote regions  

By embedding intelligence directly into local devices, we empower more secure, fast, and accessible interaction models.

---

## ğŸ“ Project Structure

```
offline-3d-llm-agent/
â”œâ”€â”€ stt/                # Speech-to-text engine
â”œâ”€â”€ llm/                # Quantized LLM interface
â”œâ”€â”€ tts/                # Text-to-speech engine
â”œâ”€â”€ avatar/             # 3D avatar and lip sync
â”œâ”€â”€ demo/               # Demo files and recordings
â””â”€â”€ README.md           # You're here!
```

---

## ğŸ§  Future Work

- ğŸ‘¥ Multi-user support with multiple avatars
- ğŸŒ Multilingual STT/TTS/LLM integration
- ğŸ§ª Performance benchmarking across devices
- ğŸ” On-device user profile adaptation

---

